{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = gensim.downloader.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "raw_data_fd = open('raw_data.json')\n",
    "raw_data = json.load(raw_data_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation Fn's\n",
    "def example(row):\n",
    "    print(row.name)\n",
    "    return row\n",
    "\n",
    "\n",
    "def question_matching(row):\n",
    "    keywords = {\n",
    "        'q0': set(['load', 'dataset', 'csv', 'file']),\n",
    "        'q1': set(['shape', 'summary', 'head', 'map', 'missing', 'label']),\n",
    "        'q2': set(['shuffle', 'seperate', 'split', 'training', '80', '20']),\n",
    "        'q3': set(['correlation', 'feature', 'selection', 'hypothetical']),\n",
    "        'q4': set(['hyperparameter', 'tune', 'gridsearchcv']),\n",
    "        'q5': set(['retrain', 'hyperparameter', 'decision', 'tree', 'plot']),\n",
    "        'q6': set(['predict', 'classification', 'accuracy', 'confusion', 'matrix']),\n",
    "        'q7': set(['information', 'gain', 'entropy', 'formula'])\n",
    "    }\n",
    "    name = row.name\n",
    "    prompt_answer_pairs = raw_data.get(name)\n",
    "\n",
    "    question_dict = {'q0': 0, 'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0, 'q6': 0, 'q7': 0}\n",
    "    for pair in prompt_answer_pairs:\n",
    "        prompt_set = set(pair[0].split())\n",
    "        match_counts = {key: 0 for key in keywords}\n",
    "\n",
    "        for question_key, keywords_set in keywords.items():\n",
    "            match_counts[question_key] += len(prompt_set.intersection(keywords_set))\n",
    "\n",
    "        question_label = max(match_counts, key=match_counts.get)\n",
    "        question_dict[question_label] += 1\n",
    "\n",
    "    for i in range(0, 8):\n",
    "        row[f'question_match_{i}'] = question_dict[f'q{i}']\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def length_and_count(row):\n",
    "    prompt_sum_of_words = 0\n",
    "    answer_sum_of_words = 0\n",
    "    for prompt, answer in raw_data[row.name]:\n",
    "        prompt_sum_of_words += len(prompt)\n",
    "        answer_sum_of_words += len(answer)\n",
    "\n",
    "    pair_count = len(raw_data[row.name])\n",
    "    row['pair_count'] = pair_count\n",
    "    row['avg_prompt_length'] = prompt_sum_of_words / pair_count\n",
    "    row['avg_answer_length'] = answer_sum_of_words / pair_count\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def vectorized_prompts(row):\n",
    "    key = row.name\n",
    "    prompt_answer_pairs = raw_data[key]\n",
    "    prompt_vector = np.zeros(word_to_vec.vector_size)\n",
    "\n",
    "    for each_pair in prompt_answer_pairs:\n",
    "        text = each_pair[0]\n",
    "        words = text.split()\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_to_vec:\n",
    "                word_vectors.append(word_to_vec[word])\n",
    "\n",
    "        if word_vectors:  # Calculate the average of word vectors along the columns (axis=0)\n",
    "            prompt_vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    for i, val in enumerate(prompt_vector):\n",
    "        row[f\"prompt_vector_{i}\"] = prompt_vector[i]\n",
    "    return row\n",
    "\n",
    "\n",
    "def vectorized_answers(row):\n",
    "    key = row.name\n",
    "    prompt_answer_pairs = raw_data[key]\n",
    "    prompt_vector = np.zeros(word_to_vec.vector_size)\n",
    "\n",
    "    for each_pair in prompt_answer_pairs:\n",
    "        text = each_pair[1]\n",
    "        words = text.split()\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_to_vec:\n",
    "                word_vectors.append(word_to_vec[word])\n",
    "\n",
    "        if word_vectors:  # Calculate the average of word vectors along the columns (axis=0)\n",
    "            prompt_vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    for i, val in enumerate(prompt_vector):\n",
    "        row[f\"answer_vector_{i}\"] = prompt_vector[i]\n",
    "    return row\n",
    "\n",
    "\n",
    "# Row processing\n",
    "def our_super_great_row_processor(row):\n",
    "    row = question_matching(row)\n",
    "    row = length_and_count(row)\n",
    "    row = vectorized_prompts(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"prompt_vector_{i}\" for i in range(300)]\n",
    "columns += [f\"answer_vector_{i}\" for i in range(300)]\n",
    "columns += [f\"question_match_{i}\" for i in range(8)]\n",
    "columns += [\"pair_count\", \"avg_prompt_length\", \"avg_answer_length\", \"grade\"]\n",
    "\n",
    "dataframe = pd.DataFrame(index=raw_data.keys(), columns=columns)\n",
    "dataframe.apply(our_super_great_row_processor, axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fd = open(\"./materials/scores.csv\")\n",
    "grades_csv_reader = csv.reader(grades_fd)\n",
    "\n",
    "for i, row in enumerate(grades_csv_reader):\n",
    "    if i > 0:\n",
    "        key = row[1].strip()\n",
    "        grade = float(row[2].strip())\n",
    "        dataframe.at[key, 'grade'] = grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    dataframe.drop(columns=['grade']), dataframe['grade'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_data = np.asarray(train_data).astype(np.float32)\n",
    "test_data = np.asarray(test_data).astype(np.float32)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels).astype(np.int32)\n",
    "test_labels = np.asarray(test_labels).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 10000)             6120000   \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 10000)             100010000 \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 10000)             100010000 \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 10001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206150001 (786.40 MB)\n",
      "Trainable params: 206150001 (786.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10000, input_shape=(611,)))\n",
    "model.add(Dense(10000, activation='tanh'))\n",
    "model.add(Dense(10000, activation='tanh'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
