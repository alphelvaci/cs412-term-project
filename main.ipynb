{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_fd = open('raw_data.json')\n",
    "raw_data = json.load(raw_data_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation Fn's\n",
    "def example(row):\n",
    "    print(row.name)\n",
    "    return row\n",
    "\n",
    "\n",
    "def question_matching(row):\n",
    "    keywords = {\n",
    "        'q0': set(['load', 'dataset', 'csv', 'file']),\n",
    "        'q1': set(['shape', 'summary', 'head', 'map', 'missing', 'label']),\n",
    "        'q2': set(['shuffle', 'seperate', 'split', 'training', '80', '20']),\n",
    "        'q3': set(['correlation', 'feature', 'selection', 'hypothetical']),\n",
    "        'q4': set(['hyperparameter', 'tune', 'gridsearchcv']),\n",
    "        'q5': set(['retrain', 'hyperparameter', 'decision', 'tree', 'plot']),\n",
    "        'q6': set(['predict', 'classification', 'accuracy', 'confusion', 'matrix']),\n",
    "        'q7': set(['information', 'gain', 'entropy', 'formula'])\n",
    "    }\n",
    "    name = row.name\n",
    "    prompt_answer_pairs = raw_data.get(name)\n",
    "\n",
    "    question_dict = {key: 0 for key in keywords}\n",
    "    for pair in prompt_answer_pairs:\n",
    "        prompt_set = set(pair[0].split())\n",
    "        match_counts = {key: 0 for key in keywords}\n",
    "\n",
    "        for question_key, keywords_set in keywords.items():\n",
    "            match_counts[question_key] += len(prompt_set.intersection(keywords_set))\n",
    "\n",
    "        max_match = max(match_counts.values())\n",
    "        for key, value in match_counts.items():\n",
    "            if value == max_match:\n",
    "                question_dict[key] += 1\n",
    "\n",
    "    for i in range(0, 8):\n",
    "        row[f'question_match_{i}'] = question_dict[f'q{i}']\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def length_and_count(row):\n",
    "    prompt_sum_of_words = 0\n",
    "    answer_sum_of_words = 0\n",
    "    for prompt, answer in raw_data[row.name]:\n",
    "        prompt_sum_of_words += len(prompt)\n",
    "        answer_sum_of_words += len(answer)\n",
    "\n",
    "    pair_count = len(raw_data[row.name])\n",
    "    row['pair_count'] = pair_count\n",
    "    row['avg_prompt_length'] = prompt_sum_of_words / pair_count\n",
    "    row['avg_answer_length'] = answer_sum_of_words / pair_count\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def vectorized_prompts(row):\n",
    "    key = row.name\n",
    "    prompt_answer_pairs = raw_data[key]\n",
    "    prompt_vector = np.zeros(word_to_vec.vector_size)\n",
    "\n",
    "    for each_pair in prompt_answer_pairs:\n",
    "        text = each_pair[0]\n",
    "        words = text.split()\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_to_vec:\n",
    "                word_vectors.append(word_to_vec[word])\n",
    "\n",
    "        if word_vectors:  # Calculate the average of word vectors along the columns (axis=0)\n",
    "            prompt_vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    for i, val in enumerate(prompt_vector):\n",
    "        row[f\"prompt_vector_{i}\"] = prompt_vector[i]\n",
    "    return row\n",
    "\n",
    "\n",
    "def vectorized_answers(row):\n",
    "    key = row.name\n",
    "    prompt_answer_pairs = raw_data[key]\n",
    "    prompt_vector = np.zeros(word_to_vec.vector_size)\n",
    "\n",
    "    for each_pair in prompt_answer_pairs:\n",
    "        text = each_pair[1]\n",
    "        words = text.split()\n",
    "        word_vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_to_vec:\n",
    "                word_vectors.append(word_to_vec[word])\n",
    "\n",
    "        if word_vectors:  # Calculate the average of word vectors along the columns (axis=0)\n",
    "            prompt_vector = np.mean(word_vectors, axis=0)\n",
    "\n",
    "    for i, val in enumerate(prompt_vector):\n",
    "        row[f\"answer_vector_{i}\"] = prompt_vector[i]\n",
    "    return row\n",
    "\n",
    "\n",
    "# Row processing\n",
    "def our_super_great_row_processor(row):\n",
    "    row = question_matching(row)\n",
    "    row = length_and_count(row)\n",
    "    row = vectorized_prompts(row)\n",
    "    row = vectorized_answers(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [f\"prompt_vector_{i}\" for i in range(100)]\n",
    "columns += [f\"answer_vector_{i}\" for i in range(100)]\n",
    "columns += [f\"question_match_{i}\" for i in range(8)]\n",
    "columns += [\"pair_count\", \"avg_prompt_length\", \"avg_answer_length\", \"grade\"]\n",
    "\n",
    "dataframe = pd.DataFrame(index=raw_data.keys(), columns=columns)\n",
    "dataframe.apply(our_super_great_row_processor, axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_fd = open(\"./materials/scores.csv\")\n",
    "grades_csv_reader = csv.reader(grades_fd)\n",
    "\n",
    "for i, row in enumerate(grades_csv_reader):\n",
    "    if i > 0:\n",
    "        key = row[1].strip()\n",
    "        grade = float(row[2].strip())\n",
    "        dataframe.at[key, 'grade'] = grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.apply(pd.to_numeric, errors='coerce')\n",
    "dataframe = dataframe.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    dataframe.drop(columns=['grade']), dataframe['grade'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_data = np.asarray(train_data).astype(np.float32)\n",
    "test_data = np.asarray(test_data).astype(np.float32)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels).astype(np.int32)\n",
    "test_labels = np.asarray(test_labels).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1000)              212000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10000)             10010000  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 10001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10232001 (39.03 MB)\n",
      "Trainable params: 10232001 (39.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, activation='relu', input_shape=(211,)))\n",
    "model.add(Dense(10000, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 92833.6328 - mean_squared_error: 92833.6328 - val_loss: 8123.6001 - val_mean_squared_error: 8123.6001\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 8663.0264 - mean_squared_error: 8663.0264 - val_loss: 8123.6001 - val_mean_squared_error: 8123.6001\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 8663.0264 - mean_squared_error: 8663.0264 - val_loss: 8123.6001 - val_mean_squared_error: 8123.6001\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 8663.0264 - mean_squared_error: 8663.0264 - val_loss: 8123.6001 - val_mean_squared_error: 8123.6001\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 8663.0264 - mean_squared_error: 8663.0264 - val_loss: 8123.6001 - val_mean_squared_error: 8123.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x176df6170>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    epochs=5,\n",
    "    batch_size=3,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
